#!/usr/bin/env python3
"""
ğŸ”§ ENTRENAMIENTO OPTIMIZADO
- NormalizaciÃ³n de targets (% changes en lugar de precios absolutos)
- Feature scaling mejorado con features mÃ¡s estables
- ValidaciÃ³n cruzada temporal para mejor generalizaciÃ³n
"""

import os
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import joblib
import json
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import RobustScaler, StandardScaler, QuantileTransformer
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')


def load_and_combine_data():
    """Carga y combina datos de forma optimizada"""
    print("ğŸ“‚ Cargando datos histÃ³ricos...")
    
    all_data = []
    timeframes = ['15m', '1h', '4h', '1d']
    
    for tf in timeframes:
        tf_dir = Path(f"data/raw/{tf}")
        if not tf_dir.exists():
            continue
            
        files = list(tf_dir.glob("*.csv"))
        print(f"  ğŸ“Š {tf}: {len(files)} archivos")
        
        for file in files:
            try:
                df = pd.read_csv(file)
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    df.set_index('timestamp', inplace=True)
                
                symbol = file.stem.replace(f"_{tf}", "")
                df['symbol'] = symbol
                df['timeframe'] = tf
                
                all_data.append(df)
                
            except Exception as e:
                print(f"    âš ï¸  Error en {file}: {e}")
    
    if not all_data:
        print("âŒ No se pudieron cargar datos")
        return None
    
    combined = pd.concat(all_data, ignore_index=False)
    print(f"âœ… Total datos cargados: {len(combined):,} velas")
    
    return combined


def create_stable_features(df):
    """Crea features mÃ¡s estables y robustos"""
    print("ğŸ”§ Creando features estables...")
    
    # Ordenar por timestamp para mantener secuencia temporal
    df = df.sort_index()
    
    # ğŸ¯ TARGET OPTIMIZADO: % change en lugar de precio absoluto
    df['target_pct'] = df['close'].pct_change().shift(-1)  # % change futuro
    
    # ğŸ“Š FEATURES NORMALIZADOS (mÃ¡s estables)
    
    # 1. Returns normalizados
    df['returns_1'] = df['close'].pct_change()
    df['returns_3'] = df['close'].pct_change(periods=3)
    df['returns_5'] = df['close'].pct_change(periods=5)
    
    # 2. Moving averages como ratios (mÃ¡s estables)
    df['sma_5'] = df['close'].rolling(window=5).mean()
    df['sma_20'] = df['close'].rolling(window=20).mean()
    df['sma_50'] = df['close'].rolling(window=50).mean()
    
    # Ratios estables
    df['ma_ratio_5_20'] = df['sma_5'] / df['sma_20']
    df['ma_ratio_20_50'] = df['sma_20'] / df['sma_50']
    df['price_ma_ratio'] = df['close'] / df['sma_20']
    
    # 3. RSI normalizado (ya estÃ¡ entre 0-100)
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / (loss + 1e-8)
    df['rsi'] = 100 - (100 / (1 + rs))
    df['rsi_normalized'] = (df['rsi'] - 50) / 50  # Centrar en 0
    
    # 4. Volatilidad como feature estable
    df['volatility_short'] = df['returns_1'].rolling(window=10).std()
    df['volatility_long'] = df['returns_1'].rolling(window=30).std()
    df['volatility_ratio'] = df['volatility_short'] / (df['volatility_long'] + 1e-8)
    
    # 5. Volume features normalizados
    df['volume_sma'] = df['volume'].rolling(window=20).mean()
    df['volume_ratio'] = df['volume'] / (df['volume_sma'] + 1e-8)
    df['volume_ratio_log'] = np.log1p(df['volume_ratio'])  # Log para estabilizar
    
    # 6. Momentum features
    df['momentum_3'] = df['close'] / df['close'].shift(3) - 1
    df['momentum_5'] = df['close'] / df['close'].shift(5) - 1
    df['momentum_10'] = df['close'] / df['close'].shift(10) - 1
    
    # 7. High-Low features normalizados
    df['hl_ratio'] = (df['high'] - df['low']) / df['close']
    df['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)
    
    # 8. Bollinger Bands position
    bb_period = 20
    bb_std = 2
    df['bb_middle'] = df['close'].rolling(window=bb_period).mean()
    df['bb_std'] = df['close'].rolling(window=bb_period).std()
    df['bb_upper'] = df['bb_middle'] + (bb_std * df['bb_std'])
    df['bb_lower'] = df['bb_middle'] - (bb_std * df['bb_std'])
    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-8)
    
    # Features finales - todos normalizados y estables
    feature_cols = [
        'returns_1', 'returns_3', 'returns_5',
        'ma_ratio_5_20', 'ma_ratio_20_50', 'price_ma_ratio',
        'rsi_normalized',
        'volatility_ratio',
        'volume_ratio_log',
        'momentum_3', 'momentum_5', 'momentum_10',
        'hl_ratio', 'close_position', 'bb_position'
    ]
    
    # Limpiar datos
    clean_mask = ~(df[feature_cols].isnull().any(axis=1) | df['target_pct'].isnull())
    df_clean = df[clean_mask].copy()
    
    # Filtrar outliers extremos en target (> 50% change)
    target_outlier_mask = (df_clean['target_pct'].abs() < 0.5)
    df_clean = df_clean[target_outlier_mask].copy()
    
    print(f"âœ… Features estables creados: {len(feature_cols)}")
    print(f"ğŸ“Š Datos limpios: {len(df_clean):,} muestras")
    print(f"ğŸ¯ Target range: {df_clean['target_pct'].min():.4f} a {df_clean['target_pct'].max():.4f}")
    
    return df_clean, feature_cols


def cross_validate_models(X, y, feature_cols):
    """ValidaciÃ³n cruzada temporal"""
    print("\nğŸ”„ VALIDACIÃ“N CRUZADA TEMPORAL...")
    
    # TimeSeriesSplit para validaciÃ³n temporal
    tscv = TimeSeriesSplit(n_splits=5)
    
    cv_scores = {'rf': [], 'ridge': []}
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"  ğŸ“ˆ Fold {fold + 1}/5...")
        
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]
        
        # Scaling
        scaler_cv = QuantileTransformer(output_distribution='normal')
        X_train_scaled = scaler_cv.fit_transform(X_train_cv)
        X_val_scaled = scaler_cv.transform(X_val_cv)
        
        # Random Forest
        rf_cv = RandomForestRegressor(
            n_estimators=50,
            max_depth=6,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42
        )
        rf_cv.fit(X_train_scaled, y_train_cv)
        rf_pred = rf_cv.predict(X_val_scaled)
        rf_score = r2_score(y_val_cv, rf_pred)
        cv_scores['rf'].append(rf_score)
        
        # Ridge
        ridge_cv = Ridge(alpha=1.0)
        ridge_cv.fit(X_train_scaled, y_train_cv)
        ridge_pred = ridge_cv.predict(X_val_scaled)
        ridge_score = r2_score(y_val_cv, ridge_pred)
        cv_scores['ridge'].append(ridge_score)
    
    # Resultados de CV
    rf_cv_mean = np.mean(cv_scores['rf'])
    rf_cv_std = np.std(cv_scores['rf'])
    ridge_cv_mean = np.mean(cv_scores['ridge'])
    ridge_cv_std = np.std(cv_scores['ridge'])
    
    print(f"  ğŸŒ² RF CV: {rf_cv_mean:.3f} Â± {rf_cv_std:.3f}")
    print(f"  ğŸ”ï¸ Ridge CV: {ridge_cv_mean:.3f} Â± {ridge_cv_std:.3f}")
    
    return cv_scores


def train_optimized_models():
    """Entrenamiento optimizado con mejores prÃ¡cticas"""
    print("ğŸ¯ ENTRENAMIENTO OPTIMIZADO")
    print("=" * 50)
    
    # Cargar datos
    combined_data = load_and_combine_data()
    if combined_data is None:
        return False
    
    # Crear features estables
    featured_data, feature_cols = create_stable_features(combined_data)
    
    if len(featured_data) < 200:
        print("âŒ Datos insuficientes despuÃ©s de limpieza")
        return False
    
    # Preparar datos
    X = featured_data[feature_cols]
    y = featured_data['target_pct']  # Usando % change como target
    
    print(f"\nğŸ“Š DATASET OPTIMIZADO:")
    print(f"  ğŸ“ˆ Muestras: {len(X):,}")
    print(f"  ğŸ”§ Features: {len(feature_cols)}")
    print(f"  ğŸ’° SÃ­mbolos: {featured_data['symbol'].nunique()}")
    print(f"  â° Timeframes: {list(featured_data['timeframe'].unique())}")
    print(f"  ğŸ¯ Target (% change): {y.describe()}")
    
    # ValidaciÃ³n cruzada temporal
    cv_scores = cross_validate_models(X, y, feature_cols)
    
    # DivisiÃ³n temporal (80/20)
    split_point = int(len(X) * 0.8)
    X_train = X.iloc[:split_point]
    X_test = X.iloc[split_point:]
    y_train = y.iloc[:split_point]
    y_test = y.iloc[split_point:]
    
    print(f"\nğŸ“‹ DIVISIÃ“N TEMPORAL:")
    print(f"  ğŸƒ Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)")
    print(f"  ğŸ§ª Test: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)")
    
    # ğŸ”§ SCALING OPTIMIZADO: QuantileTransformer
    print(f"\nğŸ”§ SCALING AVANZADO:")
    scaler = QuantileTransformer(output_distribution='normal', n_quantiles=1000)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print("  âœ… QuantileTransformer aplicado (distribuciÃ³n normal)")
    
    print(f"\nğŸ¤– MODELOS OPTIMIZADOS:")
    print("-" * 30)
    
    # Random Forest optimizado
    print("ğŸŒ² Random Forest optimizado...")
    rf_model = RandomForestRegressor(
        n_estimators=100,
        max_depth=8,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        bootstrap=True,
        random_state=42,
        n_jobs=-1
    )
    
    rf_model.fit(X_train_scaled, y_train)
    rf_pred_train = rf_model.predict(X_train_scaled)
    rf_pred_test = rf_model.predict(X_test_scaled)
    
    rf_train_r2 = r2_score(y_train, rf_pred_train)
    rf_test_r2 = r2_score(y_test, rf_pred_test)
    rf_mae = mean_absolute_error(y_test, rf_pred_test)
    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred_test))
    
    print(f"   Train RÂ²: {rf_train_r2:.4f}")
    print(f"   Test RÂ²:  {rf_test_r2:.4f}")
    print(f"   MAE: {rf_mae:.6f} ({rf_mae*100:.3f}%)")
    print(f"   RMSE: {rf_rmse:.6f} ({rf_rmse*100:.3f}%)")
    print(f"   Overfitting: {abs(rf_train_r2 - rf_test_r2):.4f}")
    
    # Ridge optimizado
    print(f"\nğŸ”ï¸ Ridge optimizado...")
    ridge_model = Ridge(alpha=0.1, max_iter=2000)
    
    ridge_model.fit(X_train_scaled, y_train)
    ridge_pred_train = ridge_model.predict(X_train_scaled)
    ridge_pred_test = ridge_model.predict(X_test_scaled)
    
    ridge_train_r2 = r2_score(y_train, ridge_pred_train)
    ridge_test_r2 = r2_score(y_test, ridge_pred_test)
    ridge_mae = mean_absolute_error(y_test, ridge_pred_test)
    ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred_test))
    
    print(f"   Train RÂ²: {ridge_train_r2:.4f}")
    print(f"   Test RÂ²:  {ridge_test_r2:.4f}")
    print(f"   MAE: {ridge_mae:.6f} ({ridge_mae*100:.3f}%)")
    print(f"   RMSE: {ridge_rmse:.6f} ({ridge_rmse*100:.3f}%)")
    print(f"   Overfitting: {abs(ridge_train_r2 - ridge_test_r2):.4f}")
    
    # Ensemble optimizado
    ensemble_pred = (rf_pred_test + ridge_pred_test) / 2
    ensemble_r2 = r2_score(y_test, ensemble_pred)
    ensemble_mae = mean_absolute_error(y_test, ensemble_pred)
    ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))
    
    print(f"\nğŸ§  Ensemble optimizado:")
    print(f"   RÂ²: {ensemble_r2:.4f}")
    print(f"   MAE: {ensemble_mae:.6f} ({ensemble_mae*100:.3f}%)")
    print(f"   RMSE: {ensemble_rmse:.6f} ({ensemble_rmse*100:.3f}%)")
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ” TOP 5 FEATURES:")
    for i, row in feature_importance.head().iterrows():
        print(f"   {row['feature']}: {row['importance']:.3f}")
    
    # Guardar modelos optimizados
    print(f"\nğŸ’¾ Guardando modelos optimizados...")
    models_dir = Path("data/models")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    joblib.dump(rf_model, models_dir / "rf_optimized.joblib")
    joblib.dump(ridge_model, models_dir / "ridge_optimized.joblib")
    joblib.dump(scaler, models_dir / "scaler_optimized.joblib")
    
    # InformaciÃ³n del modelo optimizado
    model_info = {
        'model_type': 'optimized_percentage_prediction',
        'target_type': 'percentage_change',
        'feature_columns': feature_cols,
        'scaling_method': 'QuantileTransformer',
        'cross_validation': {
            'rf_scores': cv_scores['rf'],
            'ridge_scores': cv_scores['ridge'],
            'rf_mean': np.mean(cv_scores['rf']),
            'ridge_mean': np.mean(cv_scores['ridge'])
        },
        'models': {
            'random_forest': {
                'train_r2': rf_train_r2,
                'test_r2': rf_test_r2,
                'mae': rf_mae,
                'rmse': rf_rmse,
                'overfitting': abs(rf_train_r2 - rf_test_r2)
            },
            'ridge': {
                'train_r2': ridge_train_r2,
                'test_r2': ridge_test_r2,
                'mae': ridge_mae,
                'rmse': ridge_rmse,
                'overfitting': abs(ridge_train_r2 - ridge_test_r2)
            },
            'ensemble': {
                'r2': ensemble_r2,
                'mae': ensemble_mae,
                'rmse': ensemble_rmse
            }
        },
        'feature_importance': feature_importance.to_dict('records'),
        'training_date': datetime.now().isoformat(),
        'data_info': {
            'total_samples': len(X),
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'symbols': featured_data['symbol'].nunique(),
            'timeframes': list(featured_data['timeframe'].unique()),
            'target_stats': {
                'mean': float(y.mean()),
                'std': float(y.std()),
                'min': float(y.min()),
                'max': float(y.max())
            }
        }
    }
    
    with open(models_dir / "optimized_model_info.json", 'w') as f:
        json.dump(model_info, f, indent=2)
    
    print("âœ… Modelos optimizados guardados")
    
    # EvaluaciÃ³n final
    print(f"\nğŸ” EVALUACIÃ“N OPTIMIZADA:")
    print("-" * 25)
    
    def evaluate_optimized_model(test_r2, mae_pct, model_name):
        print(f"\n{model_name}:")
        
        if test_r2 > 0.1:
            r2_status = "ğŸ‰ EXCELENTE"
        elif test_r2 > 0.05:
            r2_status = "âœ… BUENO"
        elif test_r2 > 0.01:
            r2_status = "âš ï¸ MODERADO"
        else:
            r2_status = "âŒ BAJO"
        
        if mae_pct < 1.0:
            mae_status = "ğŸ¯ PRECISIÃ“N ALTA"
        elif mae_pct < 2.0:
            mae_status = "âœ… PRECISIÃ“N BUENA"
        elif mae_pct < 5.0:
            mae_status = "âš ï¸ PRECISIÃ“N MODERADA"
        else:
            mae_status = "âŒ PRECISIÃ“N BAJA"
        
        print(f"  RÂ²: {test_r2:.4f} {r2_status}")
        print(f"  MAE: {mae_pct:.3f}% {mae_status}")
        
        return test_r2 > 0.01 and mae_pct < 5.0
    
    rf_ok = evaluate_optimized_model(rf_test_r2, rf_mae*100, "ğŸŒ² Random Forest")
    ridge_ok = evaluate_optimized_model(ridge_test_r2, ridge_mae*100, "ğŸ”ï¸ Ridge")
    ensemble_ok = evaluate_optimized_model(ensemble_r2, ensemble_mae*100, "ğŸ§  Ensemble")
    
    # Resultado final
    print(f"\nğŸ¯ RESULTADO OPTIMIZADO:")
    print("-" * 20)
    
    best_r2 = max(rf_test_r2, ridge_test_r2, ensemble_r2)
    best_mae = min(rf_mae, ridge_mae, ensemble_mae) * 100
    
    if best_r2 > 0.1 and best_mae < 2.0:
        print(f"ğŸš€ Â¡OPTIMIZACIÃ“N EXITOSA!")
        print(f"ğŸ“ˆ RÂ² = {best_r2:.4f} (explica {best_r2*100:.1f}% varianza)")
        print(f"ğŸ¯ MAE = {best_mae:.2f}% (error promedio)")
        print(f"âœ¨ Predicciones estables y realistas")
        success = True
    elif best_r2 > 0.05:
        print(f"âœ… OPTIMIZACIÃ“N BUENA")
        print(f"ğŸ“ˆ RÂ² = {best_r2:.4f}")
        print(f"ğŸ¯ MAE = {best_mae:.2f}%")
        print(f"ğŸ”§ Listo para producciÃ³n")
        success = True
    else:
        print(f"âš ï¸ OPTIMIZACIÃ“N PARCIAL")
        print(f"ğŸ“ˆ RÂ² = {best_r2:.4f}")
        print(f"ğŸ¯ MAE = {best_mae:.2f}%")
        print(f"ğŸ”„ Considera mÃ¡s datos o features")
        success = False
    
    return success


if __name__ == "__main__":
    success = train_optimized_models()
    if success:
        print(f"\nğŸ‰ Â¡ENTRENAMIENTO OPTIMIZADO EXITOSO!")
        print(f"ğŸ”§ Predicciones en % change (mÃ¡s realistas)")
        print(f"âš¡ Listo para bot optimizado")
    else:
        print(f"\nâŒ Necesita mÃ¡s optimizaciÃ³n")
        exit(1)
